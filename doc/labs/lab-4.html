<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta name="generator" content=
  "HTML Tidy for Linux/x86 (vers 12 April 2005), see www.w3.org" />
  <link rel="StyleSheet" href="style.css" type="text/css" />

  <title>6.824 Lab 4: Caching locks</title>
</head>

<body>
  <div align="center">
    <h2><a href="../index.html">6.824</a> - Spring 2012</h2>
  </div>

  <div align="center">
      <h1>6.824 Lab 4: Caching locks</h1>
  </div>

  <div align="center">
    <h3>Due: Friday, March 16th, 5:00pm</h3>
  </div>
  <hr />

<h3>Introduction</h3>

<p>In this lab you will build a lock server and client that cache
locks at the client, reducing the load on the server and improving
client performance. For example, suppose that an application using YFS
creates 100 files in a directory. Your Lab 3 <tt>yfs_client</tt> will
probably send 100 <tt>acquire</tt> and <tt>release</tt> RPCs for 
the directory's lock.
This lab will modify the lock client and server so that the lock
client sends (in the common case) just one <tt>acquire</tt> RPC for the
directory's lock, and caches the lock thereafter, only releasing
it if another <tt>yfs_client</tt> needs it.

<p>The challenge in the lab is the protocol between the clients and
the server. For example, when client 2 acquires a lock that client 1
has cached, the server must revoke that lock from client 1 by sending
a <tt>revoke</tt> RPC to client 1. The server can give client 2 the lock 
only after client 1 has released the lock, which may be a long time after
sending the revoke (e.g., if a thread on client 1 holds the lock for a
long period).  The protocol is further complicated by the fact that
concurrent RPC requests and replies may not be delivered in the
same order in which they were sent.

<p>We'll test your caching lock server and client by seeing whether it
reduces the amount of lock RPC traffic that your <tt>yfs_client</tt>
generates. We will test with both RPC_LOSSY set to 0 and RPC_LOSSY set
to 5.</p>

<h3>Getting Started</h3>

Since you are building on the past labs, ensure the code in your Lab 3
directory passes all tests for Labs 1, 2 and 3 before starting 
this lab.

<p>Begin by merging your solution to lab 3 with the new code for lab 4:

<pre>
% <b>cd ~/lab</b>
% <b>git commit -am 'my solution to lab3'</b>
Created commit ...
% <b>git pull</b>
remote: Generating pack...
...
% <b>git checkout -b lab4 origin/lab4</b>
Branch lab5 set up to track remote branch refs/remotes/origin/lab5.
Switched to a new branch "lab4"
% <b>git merge lab3</b>
</pre>

<p>This will add these new files to your <tt>lab</tt> directory:

<ul>
<li><tt>lock_client_cache.{cc,h}</tt>: This will be the new lock
client class that the <tt>lock_tester</tt> and your
<tt>yfs_client</tt> should instantiate.
<tt>lock_client_cache</tt> must
receive <tt>revoke</tt> RPCs from the server (as well as <tt>retry</tt> RPCs,
explained below), so
we have provided you with 
code in the <tt>lock_client_cache</tt> constructor that picks a random
port to listen on, creates an <tt>rpcs</tt> for that port, and constructs
an <tt>id</tt> string with the client's IP address and port
that the client can send to the server when
requesting a lock.

<p>Note that although <tt>lock_client_cache</tt> extends the
<tt>lock_client</tt> class from Lab 1, you probably won't be able to
reuse any code from the parent class; we use a subclass here so that
<tt>yfs_client</tt> can use the two implementations interchangeably.
However, you might find some member variables useful (such as
<tt>lock_client</tt>'s RPC client <tt>cl</tt>).

<li><tt>lock_server_cache.{cc,h}</tt>: Similarly, you will not
necessarily be able to use any code
from <tt>lock_server</tt>.
<tt>lock_server_cache</tt> should be instantiated
by <tt>lock_smain.cc</tt>, which should also register the RPC handlers
for the new class.

<p>
<li><tt>handle.{cc,h}</tt>: this class maintains a cache of RPC
connections to other servers. You will find it useful in your
<tt>lock_server_cache</tt> when sending <tt>revoke</tt> and <tt>retry</tt> 
RPCs to lock clients. Look at the comments at the start of <tt>handle.h</tt>.
You can pass the lock client's <tt>id</tt> string to <tt>handle</tt>
to tell it which lock client to talk to.

<p>
<li><tt>tprintf.h</tt>: this file defines a macro that prints
out the time when a printf is invoked. You may find this helpful
in debugging distributed deadlocks.

</ul>

<p>
We have also made changes to the following files:

<ul>

<li><tt>GNUmakefile</tt>: links <tt>lock_client_cache</tt>
into <tt>lock_tester</tt> and <tt>yfs_client</tt>, and link
<tt>lock_server_cache</tt> into <tt>lock_server</tt>.

<li><tt>lock_tester.cc</tt>: creates
<tt>lock_client_cache</tt> objects.

<li><tt>lock_protocol.h</tt>: 
Contains a
second protocol for the RPCs that the lock server sends to the client.

<li><tt>lock_smain.cc</tt>: #include <tt>lock_server_cache.h</tt>
  instead of <tt>lock_server.h</tt>

</ul>

<h3>Step One: Design the Protocol</h3>

Your lock client and lock server will each keep some state about
each lock, and will have a protocol by which they change that
state. Start by making a design (on paper) of the states, protocol,
and how the protocol messages generate state transitions.
Do this before you implement anything (though be prepared to
change your mind in light of experience).

<p>Here is the set of states we recommend for the client:
<ul>
<li><i>none</i>: client knows nothing about this lock
<li><i>free</i>: client owns the lock and no thread has it 
<li><i>locked</i>: client owns the lock and a thread has it
<li><i>acquiring</i>: the client is acquiring ownership
<li><i>releasing</i>: the client is releasing ownership
</ul>

<p>A single client may have multiple threads waiting for the
same lock,
but only one thread per client ever needs to be interacting with
the server; once that thread has acquired and released the lock it can
wake up other threads, one of which can acquire the lock (unless the lock
has been revoked and released back to the server).  If you need a way
to identify a thread, you can use its thread id (tid), which you can
get using <tt>pthread_self()</tt>.

<p>When a client asks for a lock with an <tt>acquire</tt> RPC, the server
  grants the lock and responds with OK if the lock is not owned by
  another client (i.e., the lock is free). 
  If the lock is not free, and there are other clients
  waiting for the lock, the server responds with a RETRY. 
  Otherwise, the server sends a <tt>revoke</tt> RPC to the owner of the
  lock, and waits for the lock to be released by the owner. Finally, 
  the server sends a <tt>retry</tt> to the next waiting 
  client (if any), grants the lock and responds with OK.
<!--  At some
  point later (after another client has released the lock using a
  <tt>release</tt> RPC), the server sends the client a <tt>retry</tt>
  RPC. The <tt>retry</tt> RPC informs the client that the lock may be
  free, and therefore it ought to try another <tt>acquire</tt> RPC.-->

<p>
<font color=red>Note that RETRY and <tt>retry</tt> are two different
things.</font>  RETRY is the value the server returns for
a <tt>acquire</tt> RPC to indicate that the requested lock is not
currently available.  <tt>retry</tt> is the RPC that
the server sends the client which is scheduled to hold a previously 
requested lock next.

<p>Once a client has acquired ownership of a lock, the client caches
the lock (<i>i.e.</i>, it keeps the lock instead of sending
a <tt>release</tt> RPC to the server when a
thread releases the lock on the client). The client can grant the lock
to other threads on the same client without interacting with the
server.

<p>The server sends the client a <tt>revoke</tt> RPC to get the lock back.
This request tells the client that it should send the lock back to the
server when it releases the lock or right now if no thread on the
client is holding the lock.

<p>
The server's per-lock state should include whether it is held by
some client, the ID (host name and port number) of that client,
and the set of other clients waiting for that lock.  The server
needs to know the holding client's ID in order to send it a
revoke message when another client wants the lock.
The server needs to know the set of waiting clients in order
to send one of them a <tt>retry</tt> RPC when the holder releases the lock.

<p>
For your convenience, we have defined a new
RPC protocol called <tt>rlock_protocol</tt> in
<tt>lock_protocol.h</tt> to use when sending RPCs from the server to
the client.  This protocol contains definitions for the <tt>retry</tt> and
<tt>revoke</tt> RPCs.

<p>
Hint: don't hold any mutexes while sending an RPC. An RPC can take
a long time, and you don't want to force other threads to wait.
Worse, holding mutexes during RPCs is an easy way to generate
distributed deadlock.

<p>The following questions might help you with your design (they are
  in no particular order):
<ul>
<!--
<li>Suppose the following happens: Client A releases a lock that
  client B wants. The server sends a <tt>retry</tt> RPC to client
  B. Then, before client B can send another <tt>acquire</tt> to the
  server, client C sends an <tt>acquire</tt> to the server. What
  happens in this case?

<li>If the server receives an <tt>acquire</tt> for a lock that is
  cached at another
  client, what is that the handler going to do?  Remember the handler
  shouldn't block or <tt>invoke</tt> RPCs.
<li>If the server receives a <tt>release</tt> RPC, and there is an outstanding
  <tt>acquire</tt> from another client, what is the handler going to do?
  Remember again that the handler shouldn't block or <tt>invoke</tt> RPCs.
<li>If a thread on the client is holding a lock, and a second thread
  calls <tt>acquire()</tt>, but there has also been a <tt>revoke</tt>
  for this lock, what
  happens?  Other clients trying to acquire the lock should have a
  fair chance of obtaining it; if two threads on the current node
  keep competing for the lock, the node should not hold the lock forever.
<li>If the server grants the lock to a client and it has a number of
  other clients also interested in that lock, you may want to indicate
  in the reply to the <tt>acquire</tt> that the client should return
  the lock to the server as soon as the thread on the client calls
  <tt>release()</tt>.  How would you keep track on the client that
  the thread's <tt>release()</tt> also results in the lock being
  returned to the server? (You could alternatively send
  a <tt>revoke</tt> RPC immediately after granting the lock in this
  case.)
-->
<li>If a thread on the client is holding a lock and a second thread
  calls <tt>acquire()</tt>, what happens?  You shouldn't need to send an RPC to
  the server.

<li>How do you handle a <tt>revoke</tt> on a client when a thread on the client
  is holding the lock? How do you handle a <tt>retry</tt> showing up on the 
  client before the response on the corresponding <tt>acquire</tt>?

<p>
Hint: a client may receive a <tt>revoke</tt> RPC for a lock before it has
received an OK response from its <tt>acquire</tt> RPC. Your client code will 
need to remember the fact that the revoke has arrived, and release
the lock as soon as you are done with it. The same situation can
arise with <tt>retry</tt> RPCs, which can arrive at the client before the
corresponding acquire returns the RETRY failure code.

<li>How do you handle a <tt>revoke</tt> showing up on the client before the
  response on the corresponding <tt>acquire</tt>?

<!--
<li>When do you increase a sequence number on the client?  Do you have
  one sequence number per <tt>lock_client</tt> object or one per client
  machine?--!>


</ul>

<h3>Step Two: Lock Client and Server, and Testing with RPC_LOSSY=0</h3>

<p>A reasonable first step would be to implement the basic design of
your <tt>acquire</tt> protocol on both the client and the server, including
having the server send <tt>revoke</tt> messages to the holder of a lock if
another client requests it, and <tt>retry</tt> messages to the next
waiting client.
<!--This will involve registering RPC
handlers on the client, and devising a way for the server to receive
and remember each client's IP address and port (<i>i.e.</i>, the
<i>id</i> variable in <tt>lock_client_cache</tt>) and using it
to send the client RPCs.-->

<p>Next you'll probably want to implement the <tt>release</tt> code path on
both the client and the server.  Of course, the client should only
inform the server of the release if the lock has been revoked.  
<!--This
will also involve having the server send a <tt>retry</tt> RPC to the next
client in line waiting for the lock.-->

<p><font color="red">Also make sure you instantiate a <tt>lock_server_cache</tt> object
in <tt>lock_smain.cc</tt>, and correctly register the RPC handlers.</font>

<p>Once you have your full protocol implemented, you can run it using
the lock tester, just as in <a href="lab-1.html">Lab 1</a>.  For now,
don't bother testing with loss:

<pre>
% <b>export RPC_LOSSY=0</b>
% <b>./lock_server 3772</b>
</pre>

<p>
Then, in another terminal:

<pre>
% <b>./lock_tester 3772</b>
</pre>

<p>Run <tt>lock_tester</tt>.  You should pass all tests and see no timeouts.
You can hit Ctrl-C in the server's window to stop it.

<p>A lock client might be holding cached locks when it exits.
This may cause another run of <tt>lock_tester</tt> using the
same <tt>lock_server</tt> to fail when the lock server tries
to send revokes to the previous client. <font color="red">To avoid this problem without worrying about
cleaning up, you must restart the <tt>lock_server</tt> for each run of
<tt>lock_tester</tt>.  </font>

<h3>Step Three: Testing the Lock Client and Server with RPC_LOSSY=5</h3>

<p>Now that it works without loss, you should try testing with
<tt>RPC_LOSSY=5</tt>.  Here you may discover problems with
reordered RPCs and responses.

<pre>
% <b>export RPC_LOSSY=5</b>
% <b>./lock_server 3772</b>
</pre>

<p>
Then, in another terminal:

<pre>
% <b>export RPC_LOSSY=5</b>
% <b>./lock_tester 3772</b>
</pre>

<p>
<font color="red">Again, you must restart the <tt>lock_server</tt> for each run of
<tt>lock_tester</tt>.  </font>

<h3>Step Four: Run File System Tests</h3>

<p>In the constructor for your <tt>yfs_client</tt>, you should now
instantiate a <tt>lock_client_cache</tt> object, rather than a
<tt>lock_client</tt> object.  You will also have to include
<tt>lock_client_cache.h</tt>.  Once you do that, your YFS should just
work under all the <a href="lab-3.html">Lab 3</a> tests.  We will run
your code against all 3 tests (a, b, and c) from Lab 3.

<p>You should also compare running your YFS code with the two
different lock clients and servers, with RPC count enabled at the lock
server.  For this reason, it would be helpful to keep your Lab 3 code
around and intact, the way it was when you submitted it.
As described below,
you can turn on RPC statistics with the <tt>RPC_COUNT</tt> environment
variable.  Look for a dramatic drop in the number of <tt>acquire</tt> (0x7001)
RPCs between your Lab 3 and Lab 4 code during the <tt>test-lab-3-c</tt> test.

<p>The file system tests should pass with RPC_LOSSY set as
well. You can pass a loss parameter to <tt>start.sh</tt> and it will
enable RPC_LOSSY automatically:

<pre>
% <b>./start.sh 5</b>          # sets RPC_LOSSY to 5
</pre>

If you're having trouble, make sure that the Lab 2 tester
passes. If it doesn't, then the issues are most likely with YFS under
RPC_LOSSY, rather than your caching lock client.

<p> <font color="red">Please restart all the servers using <tt>stop.sh</tt> and
<tt>start.sh</tt> for each run of each test.  </font>

<h3>Evaluation Criteria</h3>

<p>Our measure of performance is the number of <tt>acquire</tt> RPCs sent
to your lock server while running <tt>yfs_client</tt> and
<tt>test-lab-3-c</tt>.

<p>
The RPC library has a feature that counts
unique RPCs arriving at the server.  You can set the environment variable
<tt>RPC_COUNT</tt> to <I>N</I> before you launch a server process, and it will
print out RPC statistics every <I>N</I> RPCs.  For example, in
the <tt>bash</tt> shell you could do:

<pre>
% <b>export RPC_COUNT=25</b>
% <b>./lock_server 3772</b>
RPC STATS: 7001:23 7002:2 
...
</pre>

This means that the RPC with the procedure number 0x7001
(<tt>acquire</tt> in the original <tt>lock_protocol.h</tt> file) has
been called 23 times, while RPC 0x7002 (<tt>release</tt>) has been
called twice.

<p>
<tt>test-lab-3-c</tt>
creates two subdirectories and
creates/deletes 100 files in each directory, using each directory
through only one of the two YFS clients.
You should count the <tt>acquire</tt> RPCs for your lab 3 and for your
lab 4. If your lab 4 produces a factor of 10 fewer <tt>acquire</tt> RPCs,
then you are doing a good job.
This performance goal is vague
because the exact numbers depend a bit on how you use locks
in <tt>yfs_client</tt>.

<p>
We will check the following:

<ul>
<li> Your caching lock server passes <tt>lock_tester</tt> with
  RPC_LOSSY=0 and RPC_LOSSY=5.
<li> Your file system using the caching lock client passes all the Lab
  3 tests (a, b, and c) with RPC_LOSSY=0 and RPC_LOSSY=5.
<li>Your lab 4 code generates about a tenth as many <tt>acquire</tt> RPCs
  as your lab 3 code on <tt>test-lab-3-c</tt>.
</ul>

<h3>Collaboration policy</h3>

<p>You are welcome (and encouraged!) to discuss design questions
with other students or on the 6.824 discussion list.
However, you must write all the code you hand
in for the programming assignments, except for code that we give
you as part of the assignment. You are not allowed to look at
anyone else's solution (and you're not allowed to look at
solutions from previous years). You may discuss the assignments
with other students, but you may not look at or copy each others'
code.

<h3>Handin procedure</h3>

Email your code as a gzipped tar file to <a
href="mailto:6.824-submit@pdos.csail.mit.edu">6.824-submit@pdos.csail.mit.edu</a>
by the deadline at the top of the page.  To do this, execute these
commands:

<pre>
% <b>cd ~/lab</b>
% <b>./stop.sh</b>
% <b>make clean</b>
% <b>rm core*</b>
% <b>rm *log</b>
% <b>cd ..</b>
% <b>tar czvf `whoami`-lab4.tgz lab/</b>
</pre>

or

<pre>
% <b>cd ~/6.824/lab</b>
% <b>make handin</b>
</pre>

That should produce a file called <tt>[your_user_name]-lab4.tgz</tt> in your
<tt>lab/</tt> directory.  Attach that file to an email and send it to the 6.824
submit address.
<hr />

<address>
Please post questions or comments on <a href="http://piazza.com">Piazza</a>.<br>

Back to <a href="http://pdos.csail.mit.edu/6.824/">6.824 home</a>.
</address>
  <!--  LocalWords:  ccfs filehandle RPCs cd mkdir cp wget nc cc blockdbd DBG dir
 -->
  <!--  LocalWords:  localhost blockdbc blockdb ihash fs callback
 -->
</body>
</html>
